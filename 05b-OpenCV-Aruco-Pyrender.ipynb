{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6610186-c67e-4959-b6f6-70f8da6db078",
   "metadata": {},
   "source": [
    "# Render de modelos 3D en Realidad Aumentada con Pyrender\n",
    "\n",
    "[Pyrender](https://pyrender.readthedocs.io/en/latest/index.html) es una biblioteca de Python para el render de escenas 3D que implementa [PBR (Physically Based Rendering)](https://en.wikipedia.org/wiki/Physically_based_rendering) y capaz de interpretar modelos en formato [glTF 2.0](https://www.khronos.org/gltf/) propuesto por el grupo [Khronos](https://www.khronos.org/), responsable de múltiples estándares abiertos.\n",
    "\n",
    "Pyrender actualmente presenta una ventaja y un inconveniente destacados:\n",
    "* Ventaja. Ofrece una API muy simple de usar para la carga y render de modelos.\n",
    "* Inconveniente. Su desarrollo y mantenimiento lleva unos años detenido por lo que la calidad del render de los modelos es muy básica y presenta algunos bugs.\n",
    "\n",
    "Para la carga de los modelos Pyrender hace uso de la biblioteca [trimesh](https://pypi.org/project/trimesh/) para la carga y procesado de modelos de mallas de triángulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba1a0467-ecc1-4b5f-98f6-8c144de31ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyrender\n",
    "import trimesh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceb8177-5ed9-4532-8b5e-6fb0b271eeb9",
   "metadata": {},
   "source": [
    "El renderizado de una escena 3D necesita:\n",
    "* Uno o varios modelos 3D.\n",
    "* Una o varias luces, imprescindibles para que se pueda ver el modelo. Una de las luces puede ser una luz ambiental omnipresente que no necesita que se indique su ubicación.\n",
    "* Una cámara en la que se hará la proyección de la escena.\n",
    "\n",
    "Modelos, luces y cámara deben estar ubicados en la escena en ubicaciones y posiciones relativas a un origen de coordenadas. En Realidad Aumentada, el origen de coordenadas de la escena lo ubicaremos en el centro del marcador Aruco y la cámara estará en la posición relativa de la webcam con respecto al marcador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e2e39e9-0f90-4e45-9fd1-aee7a6f5a7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import camara\n",
    "import cuia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b43b46-dc29-4822-9865-d9df7dc06a37",
   "metadata": {},
   "source": [
    "La localización, rotación y escala de los elementos de la escena 3D se especifica mediante [matrices de transformación](https://es.wikipedia.org/wiki/Matriz_de_transformaci%C3%B3n), que representan dichas operaciones en forma de matrices 4x4. Para la creación y composición de dichas matrices podemos apoyarnos en la biblioteca [mathutils](https://docs.blender.org/api/blender_python_api_current/mathutils.html), originada en el proyecto [Blender](https://www.blender.org/) para ofrecer funciones y tipos de datos que ayuden a representar elementos y operaciones necesarios en geometría 3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08bb4512-ab8f-4bea-bedd-2c3a1844de07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mathutils\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ebbb05-56d7-4e69-a268-cde444082a4b",
   "metadata": {},
   "source": [
    "**Nota**: el sistema de coordenadas usado por Pyrender (el mismo que usa OpenGL) tiene una orientación distinta al que usa OpenCV. \n",
    "\n",
    "![Sistems de coordenadas de OpenCV y Pyrender](media/opencvpyrender.png \"Sistems de coordenadas de OpenCV y Pyrender\")\n",
    "\n",
    "Por ello será necesario hacer la conversión adecuada para el uso combinado de OpenCV y Pyrender.\n",
    "\n",
    "Dada la pose percibida del marcador y calculada mediante [solvePnP](https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html#ga549c2075fac14829ff4a58bc931c033d), que tendremos especificada en forma de un vector de translación **tvec** y un vector de rotación **rvec**, necesitamos expresarla en forma de matriz de transformación y adaptarla al modelo de sistema de coordenadas de Pyrender. Esto implicará el uso de la función [Rodrigues](https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html#ga61585db663d9da06b68e70cfbf6a1eac) y la inversión de las componentes Y y Z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc97f170-f456-458b-a1ae-5ac9325a5b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fromOpencvToPyrender(rvec, tvec):\n",
    "    pose = np.eye(4)\n",
    "    pose[0:3,3] = tvec.T\n",
    "    pose[0:3,0:3] = cv2.Rodrigues(rvec)[0]\n",
    "    pose[[1,2]] *= -1\n",
    "    pose = np.linalg.inv(pose)\n",
    "    return(pose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a35869-a685-4a27-a46b-0728160a47fd",
   "metadata": {},
   "source": [
    "Esta función será la que nos indique la matriz de transformación empleada para ubicar la cámara en Pyrender (en la misma osición relativa de la webcam con respecto al marcador). El resto de elementos de la escena los ubicaremos en una posición relativa al origen de coordenadas ubicado en el centro del marcador.\n",
    "\n",
    "Para empezar necesitamos una escena a la que pondremos una luz ambiental blanca y un fondo negro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a41d0d3-4e2b-4491-836b-02b2de7bb72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "escena = pyrender.Scene(bg_color=(0,0,0), ambient_light=(1.0, 1.0, 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2270e6e3-ded6-4080-8f49-8e7279938316",
   "metadata": {},
   "source": [
    "Para cada modelo que queramos incorporar a la escena necesitamos cargarlo y ubicarlo dentro de la escena mediante una matriz de transformación.\n",
    "\n",
    "Para la carga y adaptación del modelo para que pueda ser usado por Pyrender empleamos trimesh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dbd8181-5de8-42bd-9b5d-6534becf1191",
   "metadata": {},
   "outputs": [],
   "source": [
    "nombrefi = \"media/cubo.glb\"\n",
    "modelo_trimesh = trimesh.load(nombrefi, file_type='glb')\n",
    "modelo_mesh = pyrender.Mesh.from_trimesh(list(modelo_trimesh.geometry.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8754fed-453c-40f9-9723-9962fefae4c8",
   "metadata": {},
   "source": [
    "Para obtener la matriz de transformación que indica translación, rotación y escala del modelo empleamos mathutils partiendo de una matriz identidad y componiendo en orden las transformaciones necesarias. Hay que recordar que la composición de matrices no es una operación conmutativa por lo que el orden de composición es significativo. Lo normal es aplicar en primer lugar el escalado, después la rotación y finalmente la translación. Para compensar el cambio de sistema de coordenadas se puede aplicar una rotación de 90 grados en el eje X.\n",
    "\n",
    "La matriz de transformación de translación la construimos mediante [Translation](https://docs.blender.org/api/blender_python_api_current/mathutils.html?highlight=translation#mathutils.Matrix.Translation) indicando las coordenadas X, Y y Z (expresadas en metros). Un modelo tiene su propio sistema de coordenadas local que suele tener su origen en el centro del modelo de modo que la translación que le aplicamos se traduce en la translación de su propio sistema de coordenadas local. Por ejemplo, si tenemos un modelo en forma de cubo de 10cm de lado (con su sistema de coordenadas local ubicado en su centro) y queremos situarlo sobre el centro del marcador, la matriz de transformación tendrá que situar su centro a 5cm de altura (eje Z)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "023574cf-15e8-4b60-b913-0cfe646eebfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_loc = mathutils.Matrix.Translation((0.0, 0.0, 0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17b4d09-230d-45b4-a6d7-6cf51dad8b8c",
   "metadata": {},
   "source": [
    "La matriz de transformación de rotación la construimos mediante [Rotation](https://docs.blender.org/api/blender_python_api_current/mathutils.html?highlight=rotation#mathutils.Matrix.Rotation), especificando el ángulo de rotación en radianes, el tamaño de la matriz (que en nuestro caso siempre será 4), y el eje alrededor del cual se realizará la rotación. Haremos una rotación de 90 grados en el eje X para que el modelo esté en la misma posición en la que fue definido (y así compensar el cambio de sistema de coordenadas). Si queremos otras rotaciones adicionales no tenemos más que ir definiéndolas para aplicarlas después en el orden adecuado.\n",
    "\n",
    "**Nota**: mathutils permite expresar rataciones en forma de quaterniones pero ahora nos vamos a limitar al empleo tradicional de ángulos de Euler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92b33d7a-c3c3-47f9-a49c-1d2ba4fe6925",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_rot = mathutils.Matrix.Rotation(math.radians(90.0), 4, 'X')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5032a3-1741-4bda-8ef8-30cb35acb7b6",
   "metadata": {},
   "source": [
    "La matriz de transformación de escalado se contruye mediante [Scale](https://docs.blender.org/api/blender_python_api_current/mathutils.html?highlight=scale#mathutils.Matrix.Scale) indicando el factor de escala y el tamaño de la matriz de transformación que en nuestro caso siempre será 4. Un escalado de un factor 1 dejará el modelo en el tamaño original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2157c66-f261-4b58-bca0-e122bbd5d3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_sca = mathutils.Matrix.Scale(1.0, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35585f0e-47ac-4ff2-9877-8ee352cabd4e",
   "metadata": {},
   "source": [
    "Finalmente la matriz de transformación final será fruto de la composición de las transformaciones indivicuales. Esta operación se aplica de derecha a izquierda por lo que la aplicación de scalado seguido de rotación seguido de translación se puede componer del siguiente modo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ebda729-f50a-4242-ad7e-5b6884bad5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "meshpose = mat_loc @ mat_rot @ mat_sca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f38767-67f8-47b2-b801-cc3f5dee55a7",
   "metadata": {},
   "source": [
    "Los elementos se añaden a la escena en forma de nodos indicando la matriz de transformación que expresa su pose dentro de la escena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a42ece8c-3e72-4b17-9581-644a20528934",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = pyrender.Node(mesh=modelo_mesh, matrix=meshpose) # Creamos un nodo indicando la malla y su pose\n",
    "escena.add_node(modelo) # Y la añadimos a la escena"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e048be2-00b0-4ca9-bdee-bcb52c1e364a",
   "metadata": {},
   "source": [
    "Tan solo nos falta en la escena la cámara. Pyrender permite definir cámaras ortogonales, en perspectiva y cámaras \"personalizadas\" a partir de un conjunto de parametros intrínsecos. Dado que el objetivo es mezclar de un modo adecuado mundo real y mundo virtual, necesitamos que el renderizado de la escena 3D se realice con las mismas características de la webcam. Por ello lo ideal es crear una cámara intrínseca con las mismas características que la webcam.\n",
    "\n",
    "La cámara la crearemos usando [IntrinsicsCamera(fx, fy, cx, cy)](https://pyrender.readthedocs.io/en/latest/generated/pyrender.camera.IntrinsicsCamera.html), donde *fx* y *fy* son los [campos de visión (FOV)](https://es.wikipedia.org/wiki/Campo_de_visi%C3%B3n) y *cx* y *cy* indican las coordenadas del pixel central de la óptica. Estos parámetros podemos leerlos de la matriz característica de la cámara obtenida tras el proceso de calibrado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d685e237-998c-4d5c-b2d8-187a0f6a8412",
   "metadata": {},
   "outputs": [],
   "source": [
    "fx = camara.cameraMatrix[0][0]\n",
    "fy = camara.cameraMatrix[1][1]\n",
    "cx = camara.cameraMatrix[0][2]\n",
    "cy = camara.cameraMatrix[1][2]\n",
    "\n",
    "camInt = pyrender.IntrinsicsCamera(fx, fy, cx, cy)\n",
    "cam = pyrender.Node(camera=camInt)\n",
    "escena.add_node(cam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef5ab64-d418-434a-99ae-a68995f82300",
   "metadata": {},
   "source": [
    "El nodo *cam* se ha añadido a la escena sin especificar su matriz de transformación (por defecto usará una matriz identidad). Esta matriz se irá actualizando en función de la pose detectada del marcador de Aruco.\n",
    "\n",
    "Por último, Pyrender, además de ofrecer un visualizador de escenas 3D permite obtener las imágenes para ser procesadas de un modo independiente en lo que se conoce como [*screen rendering*](https://pyrender.readthedocs.io/en/latest/examples/offscreen.html#). Se trata de la función [OffscreenRenderer](https://pyrender.readthedocs.io/en/latest/generated/pyrender.offscreen.OffscreenRenderer.html#pyrender.offscreen.OffscreenRenderer) a la que hay que suministrar las dimensiones de la imagen (que haremos coincidir con las dimensiones de la imagen capturada por la cámara."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b670a3ce-7ad9-4d16-89cd-3e5dee20b9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "camId = 0\n",
    "bk = cuia.bestBackend(camId)\n",
    "ar = cuia.myVideo(camId, bk)\n",
    "hframe = ar.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "wframe = ar.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "mirender = pyrender.OffscreenRenderer(wframe, hframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19a00bd-fa8b-47c9-a985-cbfd7fd0957f",
   "metadata": {},
   "source": [
    "Una vez creado el renderizador, cada vez que queramos un render de la escena deberemos utilizar el método [render](https://pyrender.readthedocs.io/en/latest/generated/pyrender.offscreen.OffscreenRenderer.html#pyrender.offscreen.OffscreenRenderer.render). Esta función ofrece la imagen en color RGB resultado del render así como una matriz de profundidad que indica, para cada uno de los píxeles, la distancia a la que se encuentra de la cámara, con un valor negativo para identificar los píxeles del fondo. Esta matriz la usaremos para \"recortar\" la imagen que después será ubicada sobre la imagen de la cámara. Podemos implementar una función que realice este proceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1c10f67-6feb-4fc1-8484-5d08aa17d6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def realidadMixta(renderizador, frame, escena):\n",
    "    color, m = renderizador.render(escena)\n",
    "    bgr = cv2.cvtColor(color, cv2.COLOR_RGB2BGR) #convertimos la imagen de color al espacio BGR\n",
    "\n",
    "    _, m = cv2.threshold(m, 0, 1, cv2.THRESH_BINARY) #Umbralizamos la matriz de profundidad poniendo a cero los valores negativos y el resto a uno\n",
    "    m = (m*255).astype(np.uint8) #Para usarla como canal alfa necesitamos expresarla en el rango [0,255] como números enteros\n",
    "    m = np.stack((m,m,m), axis=2) #Creamos una imagen de 3 bandas repitiendo la máscara obtenida\n",
    "\n",
    "    #A continuación empleamos la máscara y su inversa para combinar la imagen del frame con la imagen generada por el render\n",
    "    inversa = cv2.bitwise_not(m)\n",
    "    pp = cv2.bitwise_and(bgr, m)\n",
    "    fondo = cv2.bitwise_and(frame, inversa)\n",
    "    res = cv2.bitwise_or(fondo, pp)\n",
    "    return(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ac293d-e30a-4ca5-bef8-615619e06c4c",
   "metadata": {},
   "source": [
    "Por último realizamos este proceso para cada frame capturado por la webcam en el que hayamos detectado el marcador adecuado. Implementaremos una función que devuelva las matrices de rotación y translación cuando se detecte un determinado marcador del que indicamos su tamaño."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99935ae4-b755-4a19-a6d6-a14a81001266",
   "metadata": {},
   "outputs": [],
   "source": [
    "diccionario = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_5X5_50)\n",
    "detector = cv2.aruco.ArucoDetector(diccionario)\n",
    "\n",
    "def detectarPose(frame, idMarcador, tam):\n",
    "    bboxs, ids, rechazados = detector.detectMarkers(frame)\n",
    "    if ids is not None:\n",
    "        for i in range(len(ids)):\n",
    "            if ids[i] == idMarcador:\n",
    "                objPoints = np.array([[-tam/2.0, tam/2.0, 0.0],\n",
    "                                      [tam/2.0, tam/2.0, 0.0],\n",
    "                                      [tam/2.0, -tam/2.0, 0.0],\n",
    "                                      [-tam/2.0, -tam/2.0, 0.0]])\n",
    "                ret, rvec, tvec = cv2.solvePnP(objPoints, bboxs[i], camara.cameraMatrix, camara.distCoeffs)\n",
    "                if ret:\n",
    "                    return((True, rvec, tvec))\n",
    "    return((False, None, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b167c5-fa6b-4d9c-b71d-8edc5a48640b",
   "metadata": {},
   "source": [
    "En definitiva el proceso que ha de realizarse para cada frame consta de los siguientes pasos:\n",
    "* Detectar el marcador y obtener la pose (translación y rotación)\n",
    "* Ubicar la cámara de la escena 3D en la posición de la webcam\n",
    "* Combinar el renderizado del modelo 3D con la imagen de la webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ae51aa9-3718-4bf6-a4b3-32e5b1c4a595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostrarModelo(frame):\n",
    "    ret, rvec, tvec = detectarPose(frame, 0, 0.19) #Buscaremos el marcador 0 impreso con 19cm de lado\n",
    "    if ret:\n",
    "        poseCamara = fromOpencvToPyrender(rvec, tvec) #Determinamos la posición de la cámara en forma de matriz de transformación de Pyrender\n",
    "        escena.set_pose(cam, poseCamara) #Ubicamos la cámara en la posición obtenido\n",
    "        frame = realidadMixta(mirender, frame, escena) #Mezclamos mundo real y mundo virtual\n",
    "    return(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f73af05e-4e26-4c03-8410-074d3969eac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar.process = mostrarModelo\n",
    "try:\n",
    "    ar.play(\"AR\", key=ord(' '))\n",
    "finally:\n",
    "    ar.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07fad55-bb92-47c3-978b-ffc4dc9c48a6",
   "metadata": {},
   "source": [
    "![Prueba de render](media/rendercubo.png \"Prueba de render\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
